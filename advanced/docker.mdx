---
title: Docker Installation
description: Deploy Inference.net nodes using Docker on Windows and Linux
---

# Docker Installation

This document provides detailed instructions to deploy Inference.net nodes using Docker. If you are unfamiliar with Docker, please refer to the [Docker documentation](https://docs.docker.com/get-started/). Join us on Discord if you need further assistance.

### Requirements
 - Windows or Linux operating system
 - Any NVIDIA GPU found in our list of [supported hardware](/getting-started/hardware)
 - Docker Desktop (Windows) or Docker Engine (Linux)

#### Linux Installation
1. Download and install [Docker Engine for Linux](https://docs.docker.com/engine/install/ubuntu/)
2. Install NVIDIA Driver using terminal:
```bash
sudo apt update
sudo apt install ubuntu-drivers-common
sudo ubuntu-drivers autoinstall
```

Or manually select and install a specific driver version:
```bash
sudo apt update
sudo apt install ubuntu-drivers-common
ubuntu-drivers devices
sudo apt install nvidia-driver-XXX  # Replace XXX with the recommended version number
```
   You can check the recommended driver version for your GPU at [NVIDIA's driver download page](https://www.nvidia.com/download/index.aspx)

## Setup Verification
   ```bash
   # Check Docker installation
   docker --version
   docker info

   # Verify NVIDIA drivers
   nvidia-smi
   
   # Check NVIDIA Docker support
   docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi

3. Install [NVIDIA Container Toolkit for Linux](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker)
4. Register a Inference.net account at [https://devnet.inference.net/register](https://devnet.inference.net/register)
5. Verify your email after registration
6. On the dashboard, navigate to the "Workers" tab on the left
7. Click "Create Worker" in the top right-hand corner
8. Enter a name for your worker, make sure "Docker" is selected, and click "Create Worker"
9. On the Worker Details page, click "Launch Worker" in the top right-hand corner
10. Open Terminal and run the Docker container with yourcode:
```bash
docker run \
  --pull=always \
  --restart=always \
  --runtime=nvidia \
  --gpus all \
  -v ~/.inference:/root/.inference \
  inferencedevnet/amd64-nvidia-inference-node:latest \
  --code <code>
```

#### Windows Installation
1. Download and install [Docker Desktop for Windows](https://www.docker.com/products/docker-desktop)
2. Download and install the [NVIDIA Driver](https://www.nvidia.com/download/index.aspx) for your GPU
3. Install [NVIDIA Container Toolkit for Windows](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker)
4. Register a Kuzco account at [https://devnet.inference.net/register](https://devnet.inference.net/register)
5. Verify your email and connect your Discord account in Settings
6. On the dashboard, navigate to the "Workers" tab on the left
7. Click "Create Worker" in the top right-hand corner
8. Enter a name for your worker, make sure "Docker" is selected, and click "Create Worker"
9. On the Worker Details page, click "Launch Worker" in the top right-hand corner
10. Open PowerShell or Windows Terminal and run the Docker container with your code:
```bash
docker run \
  --pull=always \
  --restart=always \
  --runtime=nvidia \
  --gpus all \
  -v ~/.inference:/root/.inference \
  inferencedevnet/amd64-nvidia-inference-node:latest \
  --code <code>
```

Once your node is started, you will see it enter the "Initializing" state on the dashboard. This means that your node is preparing to accept tasks. Depending on the speed of your GPU, this process may take up to 10 minutes, but generally only takes a minute or two.


### Running Multiple Containers on Multi-GPU Devices

If you have a system with multiple graphics cards (GPUs), you can run several Docker containers at the same time, with each container using a different GPU. Think of this like having multiple workers, each with their own dedicated graphics card to work with. This setup helps you make the most of your hardware.

#### Understanding GPU Selection

Your GPU(s) are numbered starting from 0 (the first GPU) to however many you installed. You can specify to Docker which GPU(s) you want each container to use.

Think of this like assigning specific tools to specific workers - you're telling each container which GPU(s) it's allowed to use.

# How to specify GPU to docker
```bash
# Let the container use all your GPUs
docker run --gpus all ...

# Let the container use specific GPUs (in this case, the first and second GPU)
docker run --gpus '"device=0,1"' ...

# Let the container use just one GPU (in this case, the first GPU)
docker run --gpus '"device=0"' ...
```
# So for multiple GPUs 

docker run -d \                  
  --pull=always \               
  --restart=always \             
  --runtime=nvidia \             
  --gpus '"device=0"' \          # To Use the first GPU
  -v ~/.inference:/root/.inference \ 
  --name inference-node-1 \      # Give container name of your choosing
  inferencedevnet/amd64-nvidia-inference-node:latest \
  --code <code1>                


docker run -d \                  
  --pull=always \               
  --restart=always \             
  --runtime=nvidia \             
  --gpus '"device=1"' \          # To Use the second GPU
  -v ~/.inference:/root/.inference \ 
  --name inference-node-1 \      # Give container name of your choosing
  inferencedevnet/amd64-nvidia-inference-node:latest \
  --code <code1>                

Do this for as many GPUs as you have


#### Managing Resources (Memory and CPU)

Just like you wouldn't want one program to use up all your computer's resources, you can set limits on how much memory and processing power each container can use:

Use the following command to specify resoureces: Remember to specify resources according to your machine specs.

```bash
docker run \
  --gpus '"device=0"'\           # Use the first GPU
  --memory=16g \                 # Allow up to 16 gigabytes of memory.  Note:Recommended minimum memory space is 30 gigabytes
  --cpus=8 \                     # Allow up to 8 CPU cores
  inferencedevnet/amd64-nvidia-inference-node:latest \
  --code <code>
```

#### Example: Running Two Containers With Resource Managing

Here's an example of running two inference nodes, each using a different GPU. 

```bash
# First container - using the first GPU (GPU 0)

docker run -d \                  # To run process in background
  --pull=always \                
  --restart=always \            
  --runtime=nvidia \             
  --gpus '"device=0"' \          # Use the first GPU
  --memory=30g \                 # Allow 30GB memory
  --cpus=8 \                     # Allow 8 CPU cores
  -v ~/.inference:/root/.inference \ 
  --name inference-node-1 \      # Give container a unique name
  inferencedevnet/amd64-nvidia-inference-node:latest \
  --code <code1>                 

# Second container - using the second GPU (GPU 1)
# (Same settings as above, but using GPU 1 instead)

docker run -d \                   # To run process in background
  --pull=always \
  --restart=always \
  --runtime=nvidia \
  --gpus '"device=1"' \           # Use the second GPU
  --memory=30g \                  # Allow 30GB memory 
  --cpus=8 \                      # Allow 8 CPU cores
  -v ~/.inference:/root/.inference \
  --name inference-node-2 \       # Give container a unique name
  inferencedevnet/amd64-nvidia-inference-node:latest \
  --code <code2>
```

## Common Issues and Solutions

   If containers fail to start:
   ```bash
   # Check Docker daemon status
   sudo systemctl status docker

   # Restart Docker daemon
   sudo systemctl restart docker

   # Check Docker logs
   sudo journalctl -fu docker
   ```

   If GPU access fails:
   ```bash
   # Verify NVIDIA runtime
   docker info | grep nvidia

   # Check GPU visibility
   nvidia-smi -L

   # Reset NVIDIA drivers (if needed)
   sudo nvidia-smi --gpu-reset
   ```

   
 ## Detailed GPU Monitoring
   ```bash
   # Check GPU usage and memory in real-time
   watch -n 1 nvidia-smi

   # Get detailed memory usage per GPU
   nvidia-smi --query-gpu=timestamp,name,memory.used,memory.total,memory.free --format=csv

   # Monitor specific GPU processes
   nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv -l 1
   ```

## Container Health Checks
   ```bash
   # Check container status
   docker ps -a

   # View container logs
   docker logs <container_name>
   
   # Follow logs in real-time
   docker logs -f <container_name>

   # Check container resource usage
   docker stats <container_name>
   ```

 ## Handling Container Failures
   ```bash
   # Stop a container gracefully
   docker stop <container_name>

   # Force stop if needed
   docker kill <container_name>

   # Remove container (to start fresh)
   docker rm <container_name>

   # Clean up unused resources
   docker system prune
   ```
 ## Automatic Restart Policies
   ```bash
   # Always restart (even after system reboot)
   docker run -d --restart always ...

   # Restart on failure (up to 5 times)
   docker run -d --restart on-failure:5 ...

   # Restart unless stopped manually
   docker run -d --restart unless-stopped ...
   ```

 ## Performance Monitoring
   ```bash
   # Monitor container metrics
   docker stats --format "table {{.Name}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.NetIO}}"

   # Check GPU temperature and power
   nvidia-smi --query-gpu=temperature.gpu,power.draw,power.limit --format=csv -l 1
   ```

Remember: Always check logs when troubleshooting issues.
