---
title: Docker Installation
description: Deploy Inference.net nodes using Docker on Windows and Linux
---

# Docker Installation

This document provides detailed instructions to deploy Inference.net nodes using Docker. If you are unfamiliar with Docker, please refer to the [Docker documentation](https://docs.docker.com/get-started/). Join us on Discord if you need further assistance.

### Requirements
 - Windows or Linux operating system
 - Any NVIDIA GPU found in our list of [supported hardware](/getting-started/hardware)
 - Docker Desktop (Windows) or Docker Engine (Linux)

### Automated Installation Script (Linux)

We provide an automated installation script that handles the entire setup process. The script will:
- Check and install system requirements (including Python dependencies)
- Install or update NVIDIA drivers (automatic or manual selection)
- Install Docker and NVIDIA Container Toolkit if not present
- Set up and run Inference.net containers
- Support single or multiple GPU configurations
- Monitor GPU health and status

#### Download and Run the Script

```bash
curl -fsSL https://raw.githubusercontent.com/raymundedgar/operator-docs/refs/heads/main/scripts/setup-inference.sh | bash
```

The script will guide you through the installation process with the following options:
- Driver Installation: Automatic or manual version selection
- Worker Code: Enter once and use for all containers
- Deployment Options:
  - Run single container using all GPUs
  - Run separate container for each GPU
  - Run containers on a specific number of GPUs (automatically assigned from GPU 0)

Before running containers, the script will:
- Check and install all dependencies (including Python packages)
- Check GPU health (temperature, utilization, memory usage)
- Verify system requirements
- Install and configure Docker if not present
- Install NVIDIA Container Toolkit if not present
- Create necessary directories

The script provides several safety features:
- Automatic dependency resolution
- GPU health monitoring
- Docker permission handling
- Container status verification
- Detailed progress information

### Manual Installation Steps

If you prefer to perform the installation manually, follow these steps:

#### Linux Installation
1. Download and install [Docker Engine for Linux](https://docs.docker.com/engine/install/ubuntu/)
2. Install NVIDIA Driver using terminal:
```bash
sudo apt update
sudo apt install ubuntu-drivers-common
sudo ubuntu-drivers autoinstall
```

Or manually select and install a specific driver version:
```bash
sudo apt update
sudo apt install ubuntu-drivers-common
ubuntu-drivers devices
sudo apt install nvidia-driver-XXX  # Replace XXX with the recommended version number
```
   You can check the recommended driver version for your GPU at [NVIDIA's driver download page](https://www.nvidia.com/download/index.aspx)

3. Install [NVIDIA Container Toolkit for Linux](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker)
4. Register a Inference.net account at [https://devnet.inference.net/register](https://devnet.inference.net/register)
5. Verify your email after registration
6. On the dashboard, navigate to the "Workers" tab on the left
7. Click "Create Worker" in the top right-hand corner
8. Enter a name for your worker, make sure "Docker" is selected, and click "Create Worker"
9. On the Worker Details page, click "Launch Worker" in the top right-hand corner
10. Open Terminal and run the Docker container with your code:
```bash
docker run \
  --pull=always \
  --restart=always \
  --runtime=nvidia \
  --gpus all \
  -v ~/.inference:/root/.inference \
  inferencedevnet/amd64-nvidia-inference-node:latest \
  --code <code>
```

#### Windows Installation
1. Download and install [Docker Desktop for Windows](https://www.docker.com/products/docker-desktop)
2. Download and install the [NVIDIA Driver](https://www.nvidia.com/download/index.aspx) for your GPU
3. Install [NVIDIA Container Toolkit for Windows](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker)
4. Register a Kuzco account at [https://devnet.inference.net/register](https://devnet.inference.net/register)
5. Verify your email and connect your Discord account in Settings
6. On the dashboard, navigate to the "Workers" tab on the left
7. Click "Create Worker" in the top right-hand corner
8. Enter a name for your worker, make sure "Docker" is selected, and click "Create Worker"
9. On the Worker Details page, click "Launch Worker" in the top right-hand corner
10. Open PowerShell or Windows Terminal and run the Docker container with your code:
```bash
docker run \
  --pull=always \
  --restart=always \
  --runtime=nvidia \
  --gpus all \
  -v ~/.inference:/root/.inference \
  inferencedevnet/amd64-nvidia-inference-node:latest \
  --code <code>
```

Once your node is started, you will see it enter the "Initializing" state on the dashboard. This means that your node is preparing to accept tasks. Depending on the speed of your GPU, this process may take up to 10 minutes, but generally only takes a minute or two.

### Monitoring and Managing Your Containers

After setting up your container(s), you can use these commands to monitor and manage them:

#### View Container Status and Logs

Check the status of all containers:
```bash
docker ps                                    # Show running containers
docker ps -a                                 # Show all containers (including stopped)
```

View logs for specific containers:
```bash
# For single GPU setup
docker logs -f inference-all-gpus            # Follow logs of the main container

# For multi-GPU setup
docker logs -f inference-gpu-0               # Follow logs of GPU 0 container
docker logs -f inference-gpu-1               # Follow logs of GPU 1 container
docker logs --tail 100 inference-gpu-0       # Show last 100 log lines for GPU 0
```

#### Container Management Commands

Stop containers:
```bash
# Stop a specific container
docker stop inference-gpu-0                  # Stop GPU 0 container
docker stop inference-all-gpus               # Stop single GPU container

# Stop all inference containers
docker stop $(docker ps -q --filter "name=inference-")
```

Start containers:
```bash
# Start a specific container
docker start inference-gpu-0                 # Start GPU 0 container
docker start inference-all-gpus              # Start single GPU container

# Start all inference containers
docker start $(docker ps -aq --filter "name=inference-")
```

Remove containers:
```bash
# Remove a specific container (must be stopped first)
docker rm inference-gpu-0                    # Remove GPU 0 container
docker rm inference-all-gpus                 # Remove single GPU container

# Remove all inference containers (must be stopped first)
docker rm $(docker ps -aq --filter "name=inference-")
```

#### Resource Monitoring

Monitor GPU usage:
```bash
nvidia-smi                                   # Show current GPU status
watch -n 1 nvidia-smi                        # Monitor GPU status every second
```

Monitor container resource usage:
```bash
docker stats                                 # Show live resource usage for all containers
docker stats inference-gpu-0                 # Show live resource usage for specific container
```

#### Troubleshooting Commands

Check container details:
```bash
docker inspect inference-gpu-0               # Show detailed container information
docker inspect -f '{{.State.Status}}' inference-gpu-0  # Show container status
```

View container network information:
```bash
docker network inspect bridge                # Show network details
```

Restart a container (useful if issues occur):
```bash
docker restart inference-gpu-0               # Restart specific container
docker restart $(docker ps -q --filter "name=inference-")  # Restart all inference containers
```

#### Container Maintenance

Update container image:
```bash
docker pull inferencedevnet/amd64-nvidia-inference-node:latest  # Pull latest image
```

Clean up unused resources:
```bash
docker system prune                          # Remove unused data
docker system prune -a                       # Remove all unused data (including images)
```

These commands will help you manage and monitor your Inference.net nodes effectively. Remember to replace container names according to your setup (inference-all-gpus for single GPU setup or inference-gpu-X for multi-GPU setup).
